# -*- coding: utf-8 -*-
"""SWAT_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O47ASZn2DcZ_HbXOYxxgA2kDMncYRRIo

# LOGISTICS

## GDrive
"""

!google-drive-ocamlfuse -cc

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

"""## Imports"""

import pandas as pd 
import os
import numpy as np
import string
import matplotlib.pyplot as plt
import datetime
import matplotlib.style as style
from numpy.random import randn
from numpy.random import seed
from scipy.stats import pearsonr
from sklearn import tree
from sklearn.datasets import load_iris
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics
from sklearn.tree.export import export_text
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus
!pip install -q pydot
from sklearn.datasets import load_iris
from sklearn import tree
import graphviz

"""## Data Loading"""

cd /gdrive/My Drive/EUNSUK

data=pd.read_csv('train_modified.csv',delimiter=',')

# split dataframe between analog and digital
valves=[]
pumps=[]
analog=[]
for cols in data.columns[1:-1]:
#   print(cols)
  if cols.startswith('MV'):
    valves.append(cols)
#     print("found a VALVE","-"*10)
  elif cols.startswith('P'):
    if not cols.startswith('PIT'):
      pumps.append(cols)
  else:
    analog.append(cols)
np.transpose(pumps)

print(valves)
print(pumps)
print(analog)

"""Plots of Digital and Analog Signals
---



---
"""

for cols in valves:
  print(data[cols].value_counts())

for cols in valves:
 
  fig=plt.figure()
  data.hist(column=cols)
  plt.xlabel(cols,fontsize=15)
  plt.ylabel("Frequency",fontsize=15)
  plt.xlim([1,2])
  plt.xticks([1,2])

for cols in pumps:
 
  fig=plt.figure()
  data.hist(column=cols)
  plt.xlabel(cols,fontsize=15)
  plt.ylabel("Frequency",fontsize=15)
  plt.xlim([1,2])
  plt.xticks([1,2])

x=data['MV101']

fig=plt.figure()
data.hist(column="MV101")
plt.xlabel("MV valve",fontsize=15)
plt.ylabel("Frequency",fontsize=15)
plt.xlim([0.0,3.0])

"""Data Types
---

['MV101', 'MV201', 'MV301', 'MV302', 'MV303', 'MV304']


['P101', 'P102', 'P201', 'P202', 'P203', 'P204', 'P205', 'P206', 'P301', 'P302', 'P401', 'P402', 'P403', 'P404', 'P501', 'P502', 'P601', 'P602', 'P603']


['FIT101', 'LIT101', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'DPIT301', 'FIT301', 'LIT301', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'FIT601']

## SAMPLING DATA
"""

#DATA POINTS
data_points_1=1
data_points_2=496800
data_range=data_points_2-data_points_1
xpoints = [i for i in range(data_range)]
#_____________SAMPLE_____________________#
sample_data=data[data_points_1:data_points_2]

feature_cols_comp = ['MV101', 'MV201', 'MV301', 'MV302', 'MV303', 'MV304',
                'P101', 'P102', 'P201', 'P202', 'P203', 'P204', 'P205', 
                'P206', 'P301', 'P302', 'P401', 'P402', 'P403', 'P404', 
                'P501', 'P502', 'P601', 'P602', 'P603',
                'FIT101', 'LIT101', 'AIT201', 'AIT202', 'AIT203', 'FIT201',
                'DPIT301', 'FIT301', 'LIT301', 'AIT401', 'AIT402', 'FIT401',
                'LIT401', 'UV401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 
                'FIT501', 'FIT502', 'FIT503', 'FIT504', 'FIT601']

len(feature_cols_comp)

"""# TREES
---



---
"""

#FUNCTION: DECISION TREE 
def generate_rtree(feature, output, depth,feature_cols):
      # Split data 70% training and 30% test
      X_train, X_test, y_train, y_test = train_test_split(feature,output, test_size=0.3, random_state=1) 

      target_value=(str(output.columns.values[0]))
      decision_tree = DecisionTreeClassifier(random_state=0, max_depth=depth)
      clf = decision_tree.fit(X_train,y_train)
      #tree.plot_tree(clf)
      
      d_tr = export_text(clf, feature_names=feature_cols)
      
      print(target_value)
      print(d_tr)
      return clf

def generate_vtree(X_feature,y_out,depth,classes,feature_cols):
      from sklearn.datasets import load_iris
      from sklearn import tree
      #iris = load_iris()
      clf = tree.DecisionTreeClassifier(random_state=0, max_depth=depth)
      clf = clf.fit(X_feature,y_out)
      import graphviz 
      dot_data = tree.export_graphviz(clf, out_file=None) 
      graph = graphviz.Source(dot_data) 
      graph.render("iris") 
      dot_data = tree.export_graphviz(clf, out_file=None, 
                           feature_names=feature_cols,  
                           class_names=classes,  
                           filled=True, rounded=True,  
                           special_characters=True)  
      graph = graphviz.Source(dot_data)  
      graph

"""## Decision Tree P101"""

feature_cols_p101 = ['MV101', 'MV201', 'MV301', 'MV302', 'MV303', 'MV304',
                 'FIT101', 'LIT101', 'AIT201', 'AIT202', 'AIT203', 'FIT201']

X_data = sample_data[feature_cols_p101] # Features
target=['P101']
y_data = sample_data[target] # Target variable
classes=["on","off"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.15,shuffle=False, random_state = 0)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

#DATA POINTS
data_points_1=1
data_points_2=49600
data_range=data_points_2-data_points_1
xpoints = [i for i in range(data_range)]
#_____________SAMPLE_____________________#
sample_plot=data[data_points_1:data_points_2]

fig, ax = plt.subplots(3,figsize=(10,10))
color = 'tab:red'
ax[0].set_xlabel('time (s)')
ax[0].set_ylabel('P101', color=color)
ax[0].plot(list(range(len(sample_plot['P101'][1000:110000]))),sample_plot['P101'][1000:110000], color=color)
ax[0].tick_params(axis='y', labelcolor=color)

color = 'tab:blue'
ax[1].set_ylabel('LIT301', color=color)  # we already handled the x-label with ax1
ax[1].plot(list(range(len(sample_plot['LIT101'][3000:110000]))),sample_plot['LIT301'][3000:110000], color=color)
ax[1].tick_params(axis='y', labelcolor=color)
fig.tight_layout()

color = 'tab:blue'
ax[2].set_ylabel('LIT101', color=color)  # we already handled the x-label with ax1
ax[2].plot(list(range(len(sample_plot['LIT101'][3000:110000]))),sample_plot['LIT101'][3000:110000], color=color)
ax[2].tick_params(axis='y', labelcolor=color)
fig.tight_layout()

plt.show()

clf=generate_rtree(X_train,y_train,2,feature_cols_p101)
print("\n","FEATURE IMPORTANCE","\n")
print(clf.feature_importances_)

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols_p101,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

from sklearn.metrics import accuracy_score
y_pred=clf.predict(X_test)

#ACCURACY of Decision Tree (P101)
print("Accuracy:",accuracy_score(y_test,y_pred))

#PLOT Predicted P101
plt.plot(list(range(len(y_pred[:1000]))),y_pred[:1000])

#PLOT Test Set 
plt.plot(list(range(len(y_test[:1000]))),y_test[:1000])

"""## Decision Tree MV101"""

feature_cols = ['FIT101','LIT101','LIT301', 'P302', 'P401', 'P402', 'P403', 'P404', 'P501', 'P502',
       'P601', 'P602', 'P603']
X_data = sample_data[feature_cols] # Features
target=['MV101']
y_data = sample_data[target] # Target variable
classes=["on","off"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.25, random_state = 0)

clf=generate_rtree(X_train,y_train,2)
print("\n","FEATURE IMPORTANCE","\n")
print(clf.feature_importances_)

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

from sklearn.metrics import accuracy_score
y_pred=clf.predict(X_test)

#ACCURACY of Decision Tree (P101)
print("Accuracy:",accuracy_score(y_test,y_pred))

#PLOT Predicted P101
plt.plot(list(range(len(y_pred[:100]))),y_pred[:100])

#PLOT Test Set 
plt.plot(list(range(len(y_test[:100]))),y_test[:100])

"""## Decision Tree P203"""

feature_cols = ['MV201','AIT201', 'AIT202', 'AIT203', 'FIT201', 'DPIT301', 'FIT301', 'P603']
X_data = sample_data[feature_cols] # Features
target=['P203']
y_data = sample_data[target] # Target variable
classes=["on","off"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.25, random_state = 0)

clf=generate_rtree(X_train,y_train,2)
print("\n","FEATURE IMPORTANCE","\n")
print(clf.feature_importances_)

dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

from sklearn.metrics import accuracy_score
y_pred=clf.predict(X_test)

#ACCURACY of Decision Tree (P101)
print("Accuracy:",accuracy_score(y_test,y_pred))

#PLOT Predicted P101
plt.plot(list(range(len(y_pred[:100]))),y_pred[:100])

#PLOT Test Set 
plt.plot(list(range(len(data['P203'][:10000]))),data['P203'][:10000])

feature_cols = ['MV201','AIT201', 'AIT202', 'AIT203', 'FIT201', 'DPIT301', 'FIT301']
X_feature = sample_data[feature_cols] # Features
target=['P203']
y_out = sample_data[target] # Target variable

generate_rtree(X_feature,y_out,2)

feature_cols = ['FIT101','LIT101', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'DPIT301', 'FIT301', 'LIT301']
X_feature = sample_data[feature_cols] # Features
target=['P203']
y_out = sample_data[target] # Target variable
classes=["1","2"]
from sklearn.datasets import load_iris
from sklearn import tree
#iris = load_iris()
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)
clf = clf.fit(X_feature,y_out)
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("iris") 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

"""## Decision Tree P205

**P205**
"""

feature_cols = ['MV201','AIT201', 'AIT202', 'AIT203', 'FIT201', 'DPIT301', 'FIT301']
X_feature = sample_data[feature_cols] # Features
target=['P205']
y_out = sample_data[target] # Target variable

generate_rtree(X_feature,y_out,2)

feature_cols = ['MV201','FIT101','LIT101', 'AIT201', 'AIT202', 'AIT203',  'DPIT301', 'FIT301', 'LIT301']
X_feature = sample_data[feature_cols] # Features
target=['P205']
y_out = sample_data[target] # Target variable
classes=["1","2"]
from sklearn.datasets import load_iris
from sklearn import tree
#iris = load_iris()
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)
clf = clf.fit(X_feature,y_out)
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("iris") 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

"""## Decision Tree P302

**P301**
"""

feature_cols = ['DPIT301', 'FIT301', 'LIT301',  'FIT401', 'LIT401']
X_feature = sample_data[feature_cols] # Features
target=['P302']
y_out = sample_data[target] # Target variable

generate_rtree(X_feature,y_out,4)

from IPython.display import Image 
feature_cols = ['DPIT301', 'FIT301', 'LIT301',  'FIT401', 'LIT401']
X_feature = sample_data[feature_cols] # Features
target=['P302']
y_out = sample_data[target] # Target variable
classes=["1","2"]
from sklearn.datasets import load_iris
from sklearn import tree
#iris = load_iris()
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=4)
clf = clf.fit(X_feature,y_out)
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
graph.render("iris") 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=classes,  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph 
# graph.write_png('/content/MV101_112.png')
# Image(graph.create_png())

"""# PLOTS

## Transition Counts
---
"""

def transition(dt):
  cur_state=0
  unique_items=dt.unique()
  uilist=unique_items.tolist()
#   print(unique_items)
#   trans_mat=[[0]*len(unique_items)]*len(unique_items)
  total_trans=0
  t0_1=0
  t0_2=0
  
  t1_0=0
  t1_2=0
  
  t2_1=0
  t2_0=0
#   print(trans_mat)
  for i in range(len(dt)):
    cur_state=dt[i]
    if i>1:
      prev_state=dt[i-1]
      if cur_state ==0 and prev_state==1:
      
        t1_0 +=1
      if cur_state ==0 and prev_state==2:
      
        t0_2 +=1
        
        
        
      if cur_state ==1 and prev_state==0:
      
        t1_0 +=1
        
      if cur_state ==1 and prev_state==2:
      
        t1_1 +=1
        
        
      if cur_state ==2 and prev_state==1:
        
        t2_1+=1
      if cur_state ==2 and prev_state==0:
        
        t2_0+=1
       
        
#         c=uilist.index(prev_state)
#         trans_mat[c]+=1
  

  print("TOTAL TRANSITIONS:", (t0_1+t0_2+t1_0+t1_2+t2_1+t2_0))
  print("t0_1: {}   t0_2: {}   t1_0: {}   t1_2: {}   t2_1: {}   t2_0: {} ".format(t0_1,t0_2,t1_0,t1_2,t2_1,t2_0))
#   print(trans_mat)

f=data['MV101']
len(f)
transition(f)

m=[[1,1,1],[2,2,2],[3,3,3]]
m[0][1]=52
print(m)

"""## PLOTTING SIGNALS
---

***P302***

2) UF Transfer Pump (P-301/ P-302) Operation
a. One duty, one standby
b. To interlock with RO Feed Tank level **(LIT-401)**

**i. Low Setpoint: 800mm  UF Pump (P-301/ P-302) ON**

**ii. High Setpoint: 1000mm  UF Pump (P-301/ P-302) OFF**

c. To interlock with UF Feed Tank Level (LIT-301)

i. Low Low Setpoint: 250mm  Alarm & UF Pump (P-301/ P-302) OFF

**d. To interlock with UF Feed Flowmeter (FIT-301)
i. Low Setpoint: 0.5m3/h  Pump P-301/ P-302 STOP**
"""

#DATA POINTS
data_points_1=1
data_points_2=49600
data_range=data_points_2-data_points_1
xpoints = [i for i in range(data_range)]
#_____________SAMPLE_____________________#
sample_plot=data[data_points_1:data_points_2]

fig, ax = plt.subplots(3,figsize=(15,15))
color = 'tab:red'
ax[0].set_xlabel('time (s)')
ax[0].set_ylabel('P203', color=color)
ax[0].plot(xpoints[:],sample_plot['P203'][:], color=color)
ax[0].tick_params(axis='y', labelcolor=color)
 

color = 'tab:blue'
ax[1].set_ylabel('FIT301', color=color)  # we already handled the x-label with ax1
ax[1].plot(xpoints[:],sample_plot['FIT301'][:], color=color)
ax[1].tick_params(axis='y', labelcolor=color)
fig.tight_layout()

color = 'tab:blue'
ax[2].set_ylabel('P302', color=color)  # we already handled the x-label with ax1
ax[2].plot(xpoints[:],sample_plot['P302'][:], color=color)
ax[2].tick_params(axis='y', labelcolor=color)
fig.tight_layout()

plt.show()

!pip install hmmlearn

sd=data[data_points_1:data_points_2]
num_col=(len(data.columns))
col_arr=data.columns
#print(data.columns[4])

uniq_ele_cols=[0]*num_col
for cols in range(num_col):
    x=data.columns[cols]
    uniq_ele_cols[cols]=x+"-"+str(data[x].nunique())
#print(uniq_ele_cols)


#COUNT BOOLEAN & SINGLE STATE VARIABLES
bool_count=0
bool_arr=[]
sing_state=0
sing_arr=[]
for cols in (uniq_ele_cols):
    #s=str(uniq_ele_cols[cols])
    x,y=cols.split('-')
    if int(y)==2 or int(y)==3:
        bool_count+=1
        bool_arr.append(x)
    if int(y)==1:
        sing_state+=1
        sing_arr.append(x)

import numpy as np
import hmmlearn

startprob = np.array([0.6, 0.3, 0.1])
transmat = np.array([[0.7, 0.2, 0.1], [0.3, 0.5, 0.2], [0.3, 0.3, 0.4]])
means = np.array([[0.0, 0.0], [3.0, -3.0], [5.0, 10.0]])
covars = np.tile(np.identity(2), (3, 1, 1))
model = hmmlearn.GaussianHMM(3, "full", startprob, transmat)
model.means_ = means
model.covars_ = covars
X, Z = model.sample(100)

"""Visual Trees
---
"""

from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=2)
clf = clf.fit(X_feature, y_out)
import graphviz 
dot_data = tree.export_graphviz(clf, out_file=None) 
graph = graphviz.Source(dot_data) 
# graph.render("iris") 
dot_data = tree.export_graphviz(clf, out_file=None, 
                     feature_names=feature_cols,  
                     class_names=["on","off"],  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph



"""## Tree from Scratch
---



---
"""

sd=data

header=sd.columns
print(header)

def class_counts(rows):
    """Counts the number of each type of example in a dataset."""
    #LABELLING
    labels=sd['MV101']
    counts = {}  # a dictionary of label -> count.
    
    for i in range(len(labels)):
        # in our dataset format, the label is always the last column
        label = labels[i]
        if label not in counts:
            counts[label] = 0
        counts[label] += 1
    return counts

len(sd['MV101'])

#######
# Demo:
class_counts(sd['P101'])
#######

def is_numeric(value):
    """Test if a value is numeric."""
    return isinstance(value, int) or isinstance(value, float)

#######
# Demo:
is_numeric(7)
# is_numeric("Red")
#######

class Question:
    """A Question is used to partition a dataset.

    This class just records a 'column number' (e.g., 0 for Color) and a
    'column value' (e.g., Green). The 'match' method is used to compare
    the feature value in an example to the feature value stored in the
    question. See the demo below.
    """

    def __init__(self, column, value):
        self.column = column
        self.value = value
#         print("SEARCHING FOR ",value, "in column",column)

    def match(self, example):
        # Compare the feature value in an example to the
        # feature value in this question.
        val = example[self.column]
        if is_numeric(val):
            return val >= self.value
        else:
            return val == self.value
    

    def __repr__(self):
        # This is just a helper method to print
        # the question in a readable format.
        condition = "=="
        if is_numeric(self.value):
            condition = ">="
        return "Is %s %s %s?" % (
            header[self.column], condition, str(self.value))

Question(4,1).column

#######
# Demo:
# Let's write a question for a numeric attribute
Question(4, 2)

# How about one for a categorical attribute
q = Question(4, 2)
q

example =  sd.loc[10]
print(example)

# Let's pick an example from the training set...
# example = sd['P101'][5]
# ... and see if it matches the question
q.match(example) # this will be true, since the first example is Green.
#######

def partition(df, question):
    """Partitions a dataset.
    

    For each row in the dataset, check if it matches the question. If
    so, add it to 'true rows', otherwise, add it to 'false rows'.
    """
    
    column= question.column
    col_name=str(df.columns[column])
    value=  question.value
  
    true_rows=df[df[col_name] > value]
    false_rows=df[df[col_name] == value]
    return true_rows, false_rows

#######
# Demo:
# Let's partition the training data based on whether rows are Red.
true_rows, false_rows = partition(sd, Question(4, 1))

def gini(rows):
    """Calculate the Gini Impurity for a list of rows.

    There are a few different ways to do this, I thought this one was
    the most concise. See:
    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
    """
    counts = class_counts(rows)
    impurity = 1
    for lbl in counts:
        prob_of_lbl = counts[lbl] / float(len(rows))
        impurity -= prob_of_lbl**2
    return impurity

gini(sd['MV101'])

def info_gain(left, right, current_uncertainty):
    """Information Gain.

    The uncertainty of the starting node, minus the weighted impurity of
    two child nodes.
    """
    p = float(len(left)) / (len(left) + len(right))
    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)

current_uncertainty = gini(sd)
current_uncertainty

"""## NEW ALL TREE"""

# Run this program on your local python 
# interpreter, provided you have installed 
# the required libraries. 

# Importing the required packages 
import numpy as np 
import pandas as pd 
from sklearn.metrics import confusion_matrix 
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 

# Function importing Dataset 
def importdata(): 
	balance_data = data
	
	# Printing the dataswet shape 
	print ("Dataset Length: ", len(balance_data)) 
	print ("Dataset Shape: ", balance_data.shape) 
	
	# Printing the dataset obseravtions 
	print ("Dataset: ",balance_data.head()) 
	return balance_data 

# Function to split the dataset 
def splitdataset(balance_data): 

	# Seperating the target variable 
	X = balance_data.values[:, 1:5] 
	Y = balance_data.values[:, 0] 

	# Spliting the dataset into train and test 
	X_train, X_test, y_train, y_test = train_test_split( 
	X, Y, test_size = 0.3, random_state = 100) 
	
	return X, Y, X_train, X_test, y_train, y_test 
	
# Function to perform training with giniIndex. 
def train_using_gini(X_train, X_test, y_train): 

	# Creating the classifier object 
	clf_gini = DecisionTreeClassifier(criterion = "gini", 
			random_state = 100,max_depth=3, min_samples_leaf=5) 

	# Performing training 
	clf_gini.fit(X_train, y_train) 
	return clf_gini 
	
# Function to perform training with entropy. 
def tarin_using_entropy(X_train, X_test, y_train): 

	# Decision tree with entropy 
	clf_entropy = DecisionTreeClassifier( 
			criterion = "entropy", random_state = 100, 
			max_depth = 3, min_samples_leaf = 5) 

	# Performing training 
	clf_entropy.fit(X_train, y_train) 
	return clf_entropy 


# Function to make predictions 
def prediction(X_test, clf_object): 

	# Predicton on test with giniIndex 
	y_pred = clf_object.predict(X_test) 
	print("Predicted values:") 
	print(y_pred) 
	return y_pred 
	
# Function to calculate accuracy 
def cal_accuracy(y_test, y_pred): 
	
	print("Confusion Matrix: ", 
		confusion_matrix(y_test, y_pred)) 
	
	print ("Accuracy : ", 
	accuracy_score(y_test,y_pred)*100) 
	
	print("Report : ", 
	classification_report(y_test, y_pred)) 

# Driver code 
def main(): 
	
	# Building Phase 
	data = importdata() 
	X, Y, X_train, X_test, y_train, y_test = splitdataset(data) 
	clf_gini = train_using_gini(X_train, X_test, y_train) 
	clf_entropy = tarin_using_entropy(X_train, X_test, y_train) 
	
	# Operational Phase 
	print("Results Using Gini Index:") 
	
	# Prediction using gini 
	y_pred_gini = prediction(X_test, clf_gini) 
	cal_accuracy(y_test, y_pred_gini) 
	
	print("Results Using Entropy:") 
	# Prediction using entropy 
	y_pred_entropy = prediction(X_test, clf_entropy) 
	cal_accuracy(y_test, y_pred_entropy) 
	
	
# Calling main function 
if __name__=="__main__": 
	main()

"""## Correlation PLOTS"""

sample_corr=sample_data[['MV101','MV201']]

sample_corr.describe()

plt.matshow(sample_corr.corr())
plt.show()

corr = sample_corr.corr()
corr.style.background_gradient(cmap='coolwarm')
# 'RdBu_r' & 'BrBG' are other good diverging colormaps

diff=data['MV101']-data['MV201']

# print(diff.sum()/len(data['MV101']))

fig, ax = plt.subplots(2,figsize=(10,10))
color = 'tab:red'
ax[0].set_xlabel('time (s)')
ax[0].set_ylabel('FIT101', color=color)
ax[0].plot(list(range(len(sample_plot['P101'][1000:6000]))),sample_plot['FIT101'][1000:6000], color=color)
ax[0].tick_params(axis='y', labelcolor=color)

color = 'tab:blue'
ax[1].set_ylabel('LIT101', color=color)  # we already handled the x-label with ax1
ax[1].plot(list(range(len(sample_plot['LIT101'][1000:6000]))),sample_plot['LIT101'][1000:6000], color=color)
ax[1].tick_params(axis='y', labelcolor=color)
fig.tight_layout()

"""# RANDOM FOREST
---



---
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn import metrics, datasets, ensemble
from sklearn import metrics
import random

#_________P302____________________________________________#
# Features
feature_cols = [ 'FIT201', 'LIT301',  'FIT401', 'LIT401']
x_data = data[feature_cols].values


# Target variable
target=['P302']
y_data = data[target].values

print(x_data.shape,y_data.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.75, random_state = 100)

print(X_train.shape,y_train.shape, X_test.shape,  y_test.shape)



clf = ensemble.RandomForestClassifier(n_estimators=3, max_depth=2,
                             random_state=0)
clf.fit(X_train, y_train.ravel())  
print(clf.feature_importances_)

y_pred=clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test,y_pred))

#_________MV101____________________________________________#
# Features
feature_cols = [ 'LIT101', 'LIT301',  'FIT401', 'LIT401']
x_data = data[feature_cols]


# Target variable
target=['MV101']
y_data = data[target]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.15, random_state = 100)

clf = ensemble.RandomForestClassifier(n_estimators=1, max_depth=2,
                             random_state=0)
clf.fit(X_train, y_train.ravel())  
print(clf.feature_importances_)

y_pred=clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test,y_pred))

clf = ensemble.RandomForestClassifier(n_estimators=3, max_depth=2,
                             random_state=0)
clf.fit(X_train, y_train.ravel())  
print(clf.feature_importances_)

y_pred=clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test,y_pred))

def print_decision_rules(rf):

    for tree_idx, est in enumerate(rf.estimators_):
        tree = est.tree_
        assert tree.value.shape[1] == 1 # no support for multi-output

        print('TREE: {}'.format(tree_idx))

        iterator = enumerate(zip(tree.children_left, tree.children_right, tree.feature, tree.threshold, tree.value))
        for node_idx, data in iterator:
            left, right, feature, th, value = data

            # left: index of left child (if any)
            # right: index of right child (if any)
            # feature: index of the feature to check
            # th: the threshold to compare against
            # value: values associated with classes            

            # for classifier, value is 0 except the index of the class to return
            class_idx = numpy.argmax(value[0])

            if left == -1 and right == -1:
                print('{} LEAF: return class={}'.format(node_idx, class_idx))
            else:
                print('{} NODE: if feature[{}] < {} then next={} else next={}'.format(node_idx, feature, th, left, right))

#Random Forest Model P302
estimator = ensemble.RandomForestClassifier(n_estimators=3, max_depth=2)
estimator.fit(X_train, y_train.ravel())
print("\n","FEATURE IMPORTANCE: ",estimator.feature_importances_,"\n")
print_decision_rules(estimator)

sample_predict=test_df[['FIT301', 'LIT301',  'FIT401', 'LIT401']]

y_pred=estimator.predict(sample_predict)
y_test=test_df[['P302']]

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

plt.plot(x,y)

plt.plot(x,sample_data[['P302']][10014:23414])

sample_predict=data[['FIT101','LIT101','P101']]
y_pred=estimator.predict(sample_predict)

plt.plot(list(range(len(y_pred))),y_pred)

plt.plot(list(range(len(data[['MV101']]))),data[['MV101']])



feature=np.array([0,0,0,0]).reshape(1,-1)
clf.predict([[2.3587740000000004,0, 0, 2]])

"""## P101 Random Forest"""

#_________P101____________________________________________#
# Features
feature_cols = [ 'LIT101','LIT301','FIT201', 'MV101','P301', 'P302', 'P401', 'P402', 'P403', 'P404', 'P501', 'P502',
       'P601', 'P602', 'P603']
x_data = data[feature_cols].values


# Target variable
target=['P101']
y_data = data[target].values

print(x_data.shape,y_data.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.20, random_state = 100)

print(X_train.shape,y_train.shape, X_test.shape,  y_test.shape)

clf = ensemble.RandomForestClassifier(n_estimators=3, max_depth=2,
                             random_state=0)
clf.fit(X_train, y_train.ravel())  
print(clf.feature_importances_)

#ACCURACY of Random Forest (P101)
y_pred=clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test,y_pred))

print("\n","FEATURE IMPORTANCE: ",clf.feature_importances_,"\n")
print_decision_rules(clf)

#PLOT Predicted P101
plt.plot(list(range(len(y_pred[:100]))),y_pred[:100])

#PLOT Test Set 
plt.plot(list(range(len(y_test[:100]))),y_test[:100])

"""## P203 Random Forest"""



"""# HMM"""

obs=data['P101'].to_numpy()
hidden1=data[['LIT101']].to_numpy()
hidden2=data[['LIT301']].to_numpy()
hidden3=data[['FIT201']].to_numpy()



# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
# %matplotlib inline

# create state space and initial state probabilities

states = ['sleeping', 'eating', 'pooping']
pi = [0.35, 0.35, 0.3]
state_space = pd.Series(pi, index=states, name='states')
print(state_space)
print(state_space.sum())

# create transition matrix
# equals transition probability matrix of changing states given a state
# matrix is size (M x M) where M is number of states

q_df = pd.DataFrame(columns=states, index=states)
q_df.loc[states[0]] = [0.4, 0.2, 0.4]
q_df.loc[states[1]] = [0.45, 0.45, 0.1]
q_df.loc[states[2]] = [0.45, 0.25, .3]

print(q_df)

q = q_df.values
print('\n', q, q.shape, '\n')
print(q_df.sum(axis=1))

from pprint import pprint 

# create a function that maps transition probability dataframe 
# to markov edges and weights

def _get_markov_edges(Q):
    edges = {}
    for col in Q.columns:
        for idx in Q.index:
            edges[(idx,col)] = Q.loc[idx,col]
    return edges

edges_wts = _get_markov_edges(q_df)
pprint(edges_wts)

# create graph object
G = nx.MultiDiGraph()

# nodes correspond to states
G.add_nodes_from(states)
print(f'Nodes:\n{G.nodes()}\n')

# edges represent transition probabilities
for k, v in edges_wts.items():
    tmp_origin, tmp_destination = k[0], k[1]
    G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)
print(f'Edges:')
pprint(G.edges(data=True))    

pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')
nx.draw_networkx(G, pos)

# create edge labels for jupyter plot but is not necessary
edge_labels = {(n1,n2):d['label'] for n1,n2,d in G.edges(data=True)}
nx.draw_networkx_edge_labels(G , pos, edge_labels=edge_labels)
nx.drawing.nx_pydot.write_dot(G, 'pet_dog_markov.dot')

